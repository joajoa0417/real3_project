#!/usr/bin/env python3
"""
Ollama Gemma3:4b Chatbot ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import uvicorn
import sys
import os
import subprocess
import time
import requests

def check_ollama_running():
    """Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def check_gemma_model():
    """Gemma3:4b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            for model in models:
                if "gemma3:4b" in model.get("name", ""):
                    return True
        return False
    except:
        return False

def install_gemma_model():
    """Gemma3:4b ëª¨ë¸ ì„¤ì¹˜"""
    print("ğŸ”§ Gemma3:4b ëª¨ë¸ì„ ì„¤ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    try:
        result = subprocess.run(
            ["ollama", "pull", "gemma3:4b"],
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        if result.returncode == 0:
            print("âœ… Gemma3:4b ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!")
            return True
        else:
            print(f"âŒ ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("âŒ ëª¨ë¸ ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼")
        return False
    except FileNotFoundError:
        print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. https://ollama.ai ì—ì„œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return False

def main():
    print("ğŸš€ Ollama Gemma3:4b Chatbot ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # Ollama ì„œë²„ í™•ì¸
    print("ğŸ“¡ Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘...")
    if not check_ollama_running():
        print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollamaë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”:")
        print("   ollama serve")
        return
    
    print("âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
    
    # Gemma3:4b ëª¨ë¸ í™•ì¸
    print("ğŸ” Gemma3:4b ëª¨ë¸ í™•ì¸ ì¤‘...")
    if not check_gemma_model():
        print("ğŸ“¦ Gemma3:4b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        install_choice = input("ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
        if install_choice == 'y':
            if not install_gemma_model():
                return
        else:
            print("âŒ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return
    else:
        print("âœ… Gemma3:4b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    # FastAPI ì„œë²„ ì‹œì‘
    print("ğŸŒ FastAPI ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“± API ë¬¸ì„œ: http://localhost:8000/docs")
    print("ğŸ”— ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    print("â¹ï¸  ì„œë²„ ì¤‘ì§€: Ctrl+C")
    print("-" * 50)
    
    try:
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=8000,
            reload=True,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 